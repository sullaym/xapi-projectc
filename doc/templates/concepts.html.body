<div class="row">
<h1>Concepts</h1>
<p>
When a virtual machine looks at its disk, it sees something which looks like
a real physical disk. This is an illusion. In reality the bytes of data
written to the &quot;virtual disk&quot; probably reside in a file in a filesystem or
in a logical volume on some physical storage system that the VM cannot see.
</p>

<p>
We call the real physical storage system a <b>Storage Repository</b> (SR).
</p>

<p>
We call the virtual disks within the SR <b>volumes</b>.
</p>

<h1>Manipulating volumes</h1>

<p>
When a VM is installed, a volume will be created. Typically this volume will
be deleted when the VM is uninstalled. The Xapi toolstack doesn't know how
to manipulate volumes on your storage system directly; instead it delegates
to &quot;Volume plugins&quot;: implementation-specific plugins which know
how to talk the storage-specific APIs. These volume plugins can be anything
from simple scripts in domain 0 to sophisticated services running somewhere
on the network.
 </p>

<p>
Consider for example a system using Linux LVM, where individual LVs are
mapped to VMs as virtual disks. The volume plugin for LVM could implement
the <b>Volume.create</b> API by simply calling
</p>
<pre>
  lvcreate -n name -L 64GiB -Z n
 </pre>
<p>
Consider another example where volumes are simple sparse files stored on an
NFS share. The volume plugin could implement the <b>Volume.create</b> API
by simply calling:
</p>
<pre>
  dd if=/dev/zero of=disk.name count=1 skip=64G
</pre>

<h1>Connecting volumes to VMs</h1>

<p>
VMs running on the Xen hypervisor use special shared-memory protocols to access
their disks and network interfaces. There are several implementations of these
protocols including:
</p>
<ul>
<li>the <a href="http://lxr.free-electrons.com/source/drivers/block/xen-blkback/blkback.c">
    Linux xen-blkback</a> kernel module</li>
<li>the <a href="https://github.com/freebsd/freebsd/blob/master/sys/dev/xen/blkback/blkback.c">
    FreeBSD blkback</a> driver</li>
<li>the <a href="https://github.com/qemu/qemu/blob/master/hw/xen/xen_backend.c">
    QEMU qdisk</a> userspace implementation</li>
<li>the <a href="http://xenserver.org/discuss-virtualization/virtualization-blog/entry/tapdisk3.html">
    XenServer tapdisk3</a> userspace implementation</li>
<li>the <a href="https://github.com/mirage/mirage-block-xen">MirageOS</a>
    userspace and kernelspace implementation</li>
</ul>
<p>
With so many implementations to choose from, which one should we use for a given
volume? This decision - and how to configure the implementation for
maximum performance - is the job of the Datapath plugin.
</p>

<h2>Disks as URIs</h2>

<p>
Every volume has one or more URIs, which describe how to access the data within
the volume. Examples include:
</p>
<ul>
<li>file:///local/block/device - a local file or block device</li>
<li>nfs://server/export/path/file.qcow2 - a remote NFS server exporting a .qcow2 format file</li>
<li>smb://server/export/path/file.vhd - a remote CIFS server exporting a .vhd format file</li>
<li>rbd://pool/volume - a volume in a Ceph storage pool</li>
<li>iscsi://target/lun - a LUN on an iSCSI array</li>
</ul>
<p>
The Xapi toolstack takes the list of URIs provided by the Volume plugin and
creates a connection between the VM and the disk. Xapi chooses a
&quot;Datapath plugin&quot; based on the URI scheme. The Datapath plugin returns
Xen-specific connection details, including choice of backend (kernel blkback
or userspace qemu) and caching options.
</p>
</div>

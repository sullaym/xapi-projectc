<div class="row">
<h1>Volume Mirroring</h1>

<p>
  <h2>Introduction</h2>
  Volume Mirroring is the process by which a the contents of a volume
  are replicated onto a remote volume while the source volume is in
  use. It is mainly used as the underlying technology to facility Xen
  Storage Motion (XSM), where a VM's disks are migrated whilst still
  running to another storage substrate.
</p>

<p>
  The underlying process by which this happens is via an NBD mirror
  process. This happens in different ways depending on the in-use disk
  backend. For Tapdisk, the tapdisk is paused and switched to become
  an NBD client of the mirror process. For Blkback, the volume must
  initially be set up to indirect the block accesses through a
  device-mapper node. Initially this device-mapper node will point at
  the actual block device in use (e.g. possibly another device-mapper
  node in the case of LVM). When mirroring mode is engaged, nbd-client
  is used to attach a new block device to the mirror process, and the
  device-mapper node is paused and redirected to point at the NBD
  block device. For qemu-qdisk, the internal block layer is capable
  of performing the block mirror itself.
</p>
<p>
  The mirror process is an NBD server that simultaneously writes to
  two locations. Every read operation is served from the primary
  disk, whereas every write operation is written both locally as well
  as to the secondary disk. The incoming write operation is not
  acknowledged as complete until it has completed on both primary
  and secondary disks.
</p>
<p>
  Meanwhile, there is a background task within the mirror process that
  writes a subset of the primary disks blocks to the secondary
  disk. This subset is decided by the volume plugin and passed into
  the mirror process via some mechanism.
</p>
<p>
  The mirror process can be queried to find out its progress, and can
  be cancelled.
</p>

<p>
  <h2>API Lifecycle</h2>

  Mirroring can only be engaged on a disk that has been attached and
  activated. If the disk is already being used by a running VM, the
  client requesting the mirroring must still call attach and activate
  in order that, should the VM be shut down, the disk will not be
  detached which would otherwise kill the mirroring process.
</p>
<p>
  The Mirror API calls are:

  <ul>
    <li>Datapath.Mirror.start dbg uri secondary_uri (returning a mirror_id)</li>
    <li>Datapath.Mirror.copy dbg uri mirror_id_optiono block_list</li>
    <li>Datapath.Mirror.stat dbg mirror_id (returning the status)</li>
    <li>Datapath.Mirror.cancel dbg mirror_id</li>
  </ul>
</p>

<p>
  The complete set of API calls the SM backend might expect to receive
  for a Volume Mirroring task is:

  <ul>
    <li><pre>Datapath.attach uri=vhd+file:///var/sr/1.vhd domain=dom100</pre></li>
    <li><pre>Datapath.activate uri=vhd+file:///var/sr/1.vhd domain=dom100</pre></li>
  </ul>

  At this point, the VM is using the disk. The mirror task then begins:

  <ul>
    <li><pre>Datapath.attach uri=vhd+file:///var/sr/1.vhd domain=dom0</pre></li>
    <li><pre>Datapath.activate uri=vhd+file:///var/sr/1.vhd domain=dom0</pre></li>
  </ul>

  The attach and activate come from dom0 to ensure that if the VM is shut down, the
  volume remains active for the purposes of the migration.

  <ul>
    <li><pre>Datapath.Mirror.start uri=vhd+file:///var/sr/1.vhd secondary=nbd://remote/sr/vdi</pre></li>
  </ul>

  The new writes to the disk are now being mirrored to the destination.

  <ul>
    <li><pre>Volume.query_block_differences uri=vhd+file:///var/sr/1.vhd base=null://</pre></li>
  </ul>

  This call is to find the blocks that have changed from a previously copied volume. If there
  was no previously copied volume, the differences from 'null' are requested, which simply returns
  the blocks that have data in. This is then fed to the Mirror.copy background process:

  <ul>
    <li><pre>Datapath.Mirror.copy mirror=mirror1 {block list}</pre></li>
  </ul>

  The status of the background copy is then queried:

  <ul>
    <li><pre>Datapath.Mirror.stat mirror=mirror1</pre></li>
  </ul>

  Once the background copy has been completed, the mirror is then kept synchronised until
  the disk is detached. If the disk has been mirrored as part of a XenServer XSM operaton,
  the deactivate and detach calls will be called for the VM first:

  <ul>
    <li><pre>Datapath.deactivate uri=file:///var/sr/1.vhd domain=dom100</pre></li>
    <li><pre>Datapath.detach uri=file:///var/sr/1.vhd domain=dom100</pre></li>
  </ul>

  And then subsequently the deactivate and detach will come in for the mirroring
  task:

  <ul>
    <li><pre>Datapath.deactivate uri=file:///var/sr/1.vhd domain=dom0</pre></li>
    <li><pre>Datapath.detach uri=file:///var/sr/1.vhd domain=dom0</pre></li>
  </ul>
  
    
  
</p>

<p>
  The block_list in Mirror.copy will be the result of a call to the
  change-block-tracking APIs.
</p>
<p>
  Mirror.stat will be used to determine whether the mirroring is still
  operational, and additionally how the background copy is going.
</p>
  <h2>Implementation via tapdisk/nbd-tool</h2>
<p>
  For tapdisk on VHDs, the datapath initially looks something like this:

  <div class="row">
    <img src="img/mirror_initial.png" alt="initial datapath" />
  </div>

  Once the mirror has been initialised, the datapath will look more
  like this:

  <div class="row">
    <img src="img/mirror_started.png" alt="mirror started" />
  </div>
    
  The datapath has been altered in between the tapdisk that is talking
  directly to the VM and the VHD.

  The nbd-tool binary is a block server that the tapdisk is now talking to via NBD.
  It performs both the mirroring and also the background copying for the Mirror.copy
  API call. Documentation for nbd-tool is provided with the binary and can be
  obtained by executing

  <pre>
    nbd-tool --help
  </pre>
</p>



  































</p>

</div>
